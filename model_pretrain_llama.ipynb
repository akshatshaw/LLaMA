{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i0wyyO2rKFhV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from model import *\n",
        "from utils import *\n",
        "# args.device = \"cpu\"\n",
        "args.vocab_size = 50257"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "omzbB-P_KtUn"
      },
      "outputs": [],
      "source": [
        "model = Model(args).to(args.device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install transformers\n",
        "# !pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYyk-kMiKeOb",
        "outputId": "739bad73-1a8a-4048-e0fe-6f00efc6bfbf"
      },
      "outputs": [],
      "source": [
        "# from transformers import AutoTokenizer, PreTrainedTokenizerFast\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"akshatshaw/LLaMA_hin1\")\\\n",
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8IMHHA1K8fv",
        "outputId": "05c2b5b9-aa98-4d7a-d67a-4a311a738279"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[11976,   228, 11976,   103, 11976,   243, 24231,   229, 28225,    99,\n",
              "         24231,   235, 11976,   113, 48077, 11976,   108, 48077, 28225,   248,\n",
              "         24231,   223, 11976,   101, 24231,   222, 28225,   245, 11976,   230,\n",
              "         28225,   255, 48077, 11976,   115, 48077, 28225,   106, 24231,   229,\n",
              "         11976,   224, 28225,   110, 11976,   123, 11976,   244, 11976,   101,\n",
              "         48077, 28225,   228, 11976,   116, 48077, 11976,   101, 28225,   105,\n",
              "         11976,   101, 48077, 11976,    97, 48077, 28225,   117, 24231,   230,\n",
              "            91]], device='cuda:0')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "start_context = \"आपके द्वारा चुनी गई भाषा में लिखना आसान बनाता है|\"\n",
        "text_to_token_ids(start_context, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpKTo1r1KljE",
        "outputId": "3f00286b-8991-4e5b-e8af-c30496928878"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output text:\n",
            " आपके द्वारा चुनी गई भाषा में लिखना आसान बनाता है| Buck conceptual perished Here Sort indicative therefore compromises?),apped\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(10)\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(start_context, tokenizer),\n",
        "    max_new_tokens=10,\n",
        "    context_size=args.max_seq_len\n",
        ")\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8LDdjJipMSS_"
      },
      "outputs": [],
      "source": [
        "file_path = \"mini.txt\"\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        " text_data = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mu6rn-xRUZYN",
        "outputId": "2bb2c7a1-b4e4-4709-b0a5-d87779683933"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Characters: 29962\n",
            "Tokens: 44917\n"
          ]
        }
      ],
      "source": [
        "total_characters = len(text_data)\n",
        "total_tokens = len(tokenizer.encode(text_data))\n",
        "print(\"Characters:\", total_characters)\n",
        "print(\"Tokens:\", total_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "G-zM8GIxUZVp"
      },
      "outputs": [],
      "source": [
        "train_ratio = 0.90\n",
        "split_idx = int(train_ratio * len(text_data))\n",
        "train_data = text_data[:split_idx]\n",
        "val_data = text_data[split_idx:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycQsWF8QUZTU",
        "outputId": "319e0d14-405e-40d3-a1ea-e97a057f51e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "175"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = ModelDataset(text_data, tokenizer, args.max_seq_len, args.max_seq_len)\n",
        "dataset.__len__()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2FMN0InYUZRE"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n",
        "train_loader = create_dataloader_v1(\n",
        " train_data,\n",
        " tokenizer,\n",
        " batch_size=2,\n",
        " max_length=args.max_seq_len,\n",
        " stride=args.max_seq_len,\n",
        " drop_last=True,\n",
        " shuffle=True,\n",
        " num_workers=0\n",
        ")\n",
        "val_loader = create_dataloader_v1(\n",
        " val_data,\n",
        " tokenizer,\n",
        " batch_size=2,\n",
        " max_length=args.max_seq_len,\n",
        " stride=args.max_seq_len,\n",
        " drop_last=False,\n",
        " shuffle=False,\n",
        " num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": true,
        "id": "_FCAmh0AWFJE"
      },
      "outputs": [],
      "source": [
        "# print(\"Train loader:\")\n",
        "# for x, y in train_loader:\n",
        "#  print(x.shape, y.shape)\n",
        "# print(\"\\nValidation loader:\")\n",
        "# for x, y in val_loader:\n",
        "#  print(x.shape, y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "U2ms8V3uUZO1"
      },
      "outputs": [],
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device= args.device):\n",
        " input_batch = input_batch.to(device)\n",
        " target_batch = target_batch.to(device)\n",
        " logits = model(input_batch)\n",
        " loss = torch.nn.functional.cross_entropy(\n",
        "    logits.flatten(0, 1), target_batch.flatten()\n",
        " )\n",
        " return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "s7UouC6RUZMh"
      },
      "outputs": [],
      "source": [
        "def calc_loss_loader(data_loader, model, device= args.device, num_batches=None):\n",
        "    total_loss = 0\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(\n",
        "                input_batch, target_batch, model, device\n",
        "            )\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches # Averages the loss over all batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Tycrjq-IUZKC",
        "outputId": "180ac214-8c5c-44cc-f14d-7a64dbc79a22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 11.00541517062065\n",
            "Validation loss: 11.01105456882053\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(10)\n",
        "with torch.no_grad():\n",
        "  train_loss = calc_loss_loader(train_loader, model)\n",
        "  val_loss = calc_loss_loader(val_loader, model)\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model_simple(model, train_loader, val_loader,optimizer, device, num_epochs,eval_freq, eval_iter, start_context, tokenizer):\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []   # tracking losses and token seen\n",
        "    tokens_seen, global_step = 0, -1\n",
        "    for epoch in range(num_epochs): # Main training loop\n",
        "        model.train()\n",
        "        for input_batch, target_batch in train_loader: # this loop iterates in batches\n",
        "            \n",
        "            # model.apply(lambda module: module.reset_kv_cache() if isinstance(module, MHA) else None)\n",
        "            \n",
        "            optimizer.zero_grad()           # reset loss gradient from previous batch iteration\n",
        "            loss = calc_loss_batch(\n",
        "                 input_batch, target_batch, model, device\n",
        "            )\n",
        "            loss.backward() # loss gradient\n",
        "            optimizer.step() # update model weights using loss gradients\n",
        "            tokens_seen += input_batch.numel()\n",
        "            global_step += 1\n",
        "            if global_step % eval_freq == 0: # Optional evaluation\n",
        "                train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                f\"Train loss {train_loss:.3f}, \"\n",
        "                f\"Val loss {val_loss:.3f}\")\n",
        "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
        "    return train_losses, val_losses, track_tokens_seen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluating the validation losses\n",
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()     # dropouts are disabled\n",
        "    with torch.no_grad(): # disablibng the graddient tracking\n",
        "        train_loss = calc_loss_loader(\n",
        "                    train_loader, model, device, num_batches=eval_iter\n",
        "            )\n",
        "        val_loss = calc_loss_loader(\n",
        "                     val_loader, model, device, num_batches=eval_iter\n",
        "            )\n",
        "    model.train()\n",
        "    return train_loss, val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generating and printing a sample text using the genearte_and_print_sample function\n",
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = args.max_seq_len\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(\n",
        "            model=model, idx=encoded,\n",
        "            max_new_tokens=50, context_size=context_size\n",
        "            )\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    print(decoded_text.replace(\"\\n\", \" \")) \n",
        "    model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ready.... Set.... Go!\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# model = GPTModel(GPT_CONFIG_124M)\n",
        "# model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(), \n",
        "    lr=0.0004, weight_decay=0.1\n",
        ")\n",
        "num_epochs = 1\n",
        "\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, args.device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=\"My name is Akshat shaw, I am a student at IIT Roorkee\", tokenizer=tokenizer\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
