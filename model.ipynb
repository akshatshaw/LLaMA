{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "class args:\n",
    "    dim: int = 4096\n",
    "    n_layers: int = 32\n",
    "    n_heads: int = 32\n",
    "    n_kv_heads: Optional[int] = None\n",
    "    vocab_size: int = -1 \n",
    "    multiple_of: int = 256\n",
    "    ffn_dim_multiplier: Optional[float] = None\n",
    "    norm_eps: float = 1e-5\n",
    "    theta: float = 10000.0\n",
    "    \n",
    "    # Needed for KV cache\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 2048\n",
    "\n",
    "    device: str = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import RMSNorm\n",
    "import RoPE\n",
    "rope = RoPE.rope(args.dim, args.max_seq_len,args.device, args.theta) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms = RMSNorm.RMSNorm(args.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    def __init__(self, args, qkv_bias = False):\n",
    "        super(MHA, self).__init__()\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        self.n_heads_q = args.n_heads\n",
    "        self.n_rep = self.n_heads_q // self.n_kv_heads\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        self.dim = args.dim\n",
    "        \n",
    "        self.W_query = nn.Linear(self.dim, self.head_dim*self.n_heads_q , bias=qkv_bias).to(\"cuda\")\n",
    "        self.W_key = nn.Linear(self.dim, self.n_kv_heads*self.head_dim, bias=qkv_bias).to(\"cuda\")\n",
    "        self.W_value = nn.Linear(self.dim, self.n_kv_heads*self.head_dim, bias=qkv_bias).to(\"cuda\")\n",
    "        self.out_proj = nn.Linear(args.n_heads*self.head_dim, self.dim).to(\"cuda\")\n",
    "        \n",
    "        self.cache_k = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim)).to(\"cuda\")\n",
    "        self.cache_v = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim)).to(\"cuda\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "        batch_size, seq_len, n_kv_heads, head_dim = x.shape\n",
    "        if n_rep == 1:\n",
    "            return x\n",
    "        return (\n",
    "        x[:, :, :, None, :]\n",
    "        .expand(batch_size, seq_len, n_kv_heads, n_rep, head_dim)\n",
    "        .reshape(batch_size, seq_len, n_kv_heads * n_rep, head_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, start_pos=0):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        # Linear transformation\n",
    "        xq , xk, xv = self.W_query(x), self.W_key(x), self.W_value(x)\n",
    "        \n",
    "        # Change shape to (batch_size, seq_len, n_heads, head_dim)\n",
    "        xq = xq.view(batch_size, seq_len, self.n_heads_q, self.head_dim)\n",
    "        xk = xk.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
    "        xv = xv.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)   \n",
    "\n",
    "        # Apply RoPE with position offset\n",
    "        xq = rope(xq)\n",
    "        xk = rope(xk)\n",
    "        \n",
    "        # Update KV cache\n",
    "        self.cache_k[:batch_size, start_pos:start_pos+seq_len] = xk\n",
    "        self.cache_v[:batch_size, start_pos:start_pos+seq_len] = xv\n",
    "        \n",
    "        # Retrieve cached keys/values including current sequence\n",
    "        keys = self.cache_k[:batch_size, :start_pos+seq_len]\n",
    "        values = self.cache_v[:batch_size, :start_pos+seq_len]\n",
    "        \n",
    "        # Repeat KV heads to match Q heads\n",
    "        keys = self.repeat_kv(keys, self.n_rep)\n",
    "        values = self.repeat_kv(values, self.n_rep)\n",
    "        \n",
    "        # Transpose for attention computation\n",
    "        xq = xq.transpose(1, 2)  # (bs, n_heads_q, seq_len, hd)\n",
    "        keys = keys.transpose(1, 2)  # (bs, n_heads_kv*rep, cache_len, hd)\n",
    "        values = values.transpose(1, 2)  # (bs, n_heads_kv*rep, cache_len, hd)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        \n",
    "        # Weighted sum of values\n",
    "        output = torch.matmul(scores, values)  # (bs, n_heads_q, seq_len, hd)\n",
    "        \n",
    "        # Combine heads and project\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
    "        return self.out_proj(output).to(self.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        hidden_dim = 4 * args.dim\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        if args.ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(args.ffn_dim_multiplier * hidden_dim)\n",
    "        # Round the hidden_dim to the nearest multiple of the multiple_of parameter\n",
    "        hidden_dim = args.multiple_of * ((hidden_dim + args.multiple_of - 1) // args.multiple_of)\n",
    "\n",
    "        self.w1 = nn.Linear(args.dim, hidden_dim, bias=False).to(args.device)\n",
    "        self.w2 = nn.Linear(hidden_dim, args.dim, bias=False).to(args.device)\n",
    "        self.w3 = nn.Linear(args.dim, hidden_dim, bias=False).to(args.device)\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # (B, Seq_Len, Dim) --> (B, Seq_Len, Hidden_Dim)\n",
    "        swish = F.silu(self.w1(x))\n",
    "        # (B, Seq_Len, Dim) --> (B, Seq_Len, Hidden_Dim)\n",
    "        x_V = self.w3(x)\n",
    "        # (B, Seq_Len, Hidden_Dim) * (B, Seq_Len, Hidden_Dim) --> (B, Seq_Len, Hidden_Dim)\n",
    "        x = swish * x_V\n",
    "        # (B, Seq_Len, Hidden_Dim) --> (B, Seq_Len, Dim)\n",
    "        x = self.w2(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.mha = MHA(args)\n",
    "        self.ffn = FeedForward(args)\n",
    "        self.rmsnorm = RMSNorm.RMSNorm(args.dim)\n",
    "        self.swish = nn.SiLU()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x_norm = self.rmsnorm(x)\n",
    "        x_mha = self.mha(x_norm)\n",
    "        x = x + x_mha\n",
    "        \n",
    "        x_norm_2 = self.rmsnorm(x)\n",
    "        x_ffn = self.ffn(x_norm_2)\n",
    "        x = x + x_ffn\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(*[Encoder(args) for _ in range(args.n_layers)])\n",
    "        \n",
    "        self.embd = nn.Embedding(args.vocab_size, args.dim).to(args.device)\n",
    "        self.norm = RMSNorm.RMSNorm(args.dim)\n",
    "        self.linear = nn.Linear(args.dim, args.vocab_size).to(args.device)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.embd(x)\n",
    "        x = self.encoder(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 2048, 4096).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 6.82 GiB is allocated by PyTorch, and 1.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(args)\n",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m, in \u001b[0;36mModel.__init__\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, args):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\u001b[38;5;241m*\u001b[39m[Encoder(args) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args\u001b[38;5;241m.\u001b[39mn_layers)])\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membd \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(args\u001b[38;5;241m.\u001b[39mvocab_size, args\u001b[38;5;241m.\u001b[39mdim)\u001b[38;5;241m.\u001b[39mto(args\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m RMSNorm\u001b[38;5;241m.\u001b[39mRMSNorm(args\u001b[38;5;241m.\u001b[39mdim)\n",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, args):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\u001b[38;5;241m*\u001b[39m[Encoder(args) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args\u001b[38;5;241m.\u001b[39mn_layers)])\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membd \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(args\u001b[38;5;241m.\u001b[39mvocab_size, args\u001b[38;5;241m.\u001b[39mdim)\u001b[38;5;241m.\u001b[39mto(args\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m RMSNorm\u001b[38;5;241m.\u001b[39mRMSNorm(args\u001b[38;5;241m.\u001b[39mdim)\n",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m, in \u001b[0;36mEncoder.__init__\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, args):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmha \u001b[38;5;241m=\u001b[39m MHA(args)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn \u001b[38;5;241m=\u001b[39m FeedForward(args)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrmsnorm \u001b[38;5;241m=\u001b[39m RMSNorm\u001b[38;5;241m.\u001b[39mRMSNorm(args\u001b[38;5;241m.\u001b[39mdim)\n",
      "Cell \u001b[1;32mIn[5], line 16\u001b[0m, in \u001b[0;36mMHA.__init__\u001b[1;34m(self, args, qkv_bias)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(args\u001b[38;5;241m.\u001b[39mn_heads\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_k \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((args\u001b[38;5;241m.\u001b[39mmax_batch_size, args\u001b[38;5;241m.\u001b[39mmax_seq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_kv_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim))\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_v \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((args\u001b[38;5;241m.\u001b[39mmax_batch_size, args\u001b[38;5;241m.\u001b[39mmax_seq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_kv_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim))\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 6.82 GiB is allocated by PyTorch, and 1.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "model = Model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
